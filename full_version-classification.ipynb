{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate,MaxPooling2D,BatchNormalization\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random,pickle,cv2\n",
    "from PIL import Image\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=['e/'+ x for x in os.listdir('../e/')]\n",
    "labels=pickle.load(open('../edata.pkl','rb'))\n",
    "random.shuffle(files)\n",
    "#val_files=files[:100]\n",
    "#train_files=files[100:]\n",
    "#pickle.dump(val_files,open('../fval_files.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files=pickle.load(open('../val_files.pkl','rb'))\n",
    "train_files=[f for f in files if f not in val_files ]\n",
    "print(len(train_files),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Encoder\n",
    "inputs = Input(shape=(299, 299,3,))\n",
    "\n",
    "reg =None# l2(l=0.00001)\n",
    "def primaryArch():\n",
    "    output = Conv2D(32, (3,3), padding='same')(inputs)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    \n",
    "    output = Conv2D(32, (3,3), padding='same')(inputs)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    output = keras.layers.MaxPool2D((2,2))(output)\n",
    "    #output=keras.layers.Dropout(0.5)(output)\n",
    "    output = Conv2D(64, (3,3), padding='same')(output)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    #output=keras.layers.Dropout(0.5)(output)\n",
    "    output = Conv2D(64, (3,3), padding='same')(output)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    output = keras.layers.MaxPool2D((2,2))(output)\n",
    "    output = Conv2D(128, (3,3), padding='same')(output)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    #output=keras.layers.Dropout(0.5)(output)\n",
    "    #output=BatchNormalization(axis=-1)(output)\n",
    "    output = Conv2D(128, (3,3), padding='same')(output)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    output = keras.layers.MaxPool2D((2,2))(output)\n",
    "    #output=keras.layers.Dropout(0.25)(output)\n",
    "    output = Conv2D(256, (3,3), padding='same')(output)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    #output=keras.layers.Dropout(0.5)(output)\n",
    "    output = Conv2D(256, (3,3), padding='same')(output)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    \n",
    "    output = keras.layers.MaxPool2D((2,2))(output)\n",
    "    #output=keras.layers.Dropout(0.25)(output)\n",
    "    output = Conv2D(512, (3,3), padding='same')(output)\n",
    "    output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    output = Conv2D(1, (1,1), padding='same')(output)\n",
    "    #output=BatchNormalization(axis=-1)(output)\n",
    "    output=Activation('relu')(output)\n",
    "    #output=keras.layers.Dropout(0.25)(output)\n",
    "    output = Reshape(([18*18]))(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "#x1=keras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=(299,299,3), pooling=None, classes=1000)\n",
    "\n",
    "output1=primaryArch()\n",
    "#output1=keras.layers.Dropout(0.2)(output1)\n",
    "output1 = Dense(162,activation='relu',kernel_regularizer=reg)(output1)\n",
    "#output1=keras.layers.Dropout(0.2)(output1)\n",
    "output1 = Dense(64,activation='relu',kernel_regularizer=reg)(output1)\n",
    "#output1=keras.layers.Dropout(0.2)(output1)\n",
    "output1 = Dense(32,activation='relu',kernel_regularizer=reg)(output1)\n",
    "#output1=keras.layers.Dropout(0.2,)(output1)\n",
    "output1 = Dense(8,activation='relu',kernel_regularizer=reg)(output1)\n",
    "#output1=keras.layers.Dropout(0.2)(output1)\n",
    "\n",
    "predictions1 = Dense(1, activation='sigmoid',kernel_regularizer=reg)(output1)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs ,outputs=predictions1)\n",
    "\n",
    "#model = Model(inputs=[input,embed_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in model.layers[:-8]:\n",
    "    x.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     31,
     39
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 15\n",
    "\n",
    "def train_gen(batch_size):\n",
    "    while True:\n",
    "        #print('tgen')\n",
    "        i=random.randint(0,len(train_files)-batch_size-1)\n",
    "        names=train_files[i:i+batch_size]\n",
    "        X_batch, Y_batch=generator(names,labels)\n",
    "        yield (X_batch, Y_batch)\n",
    "        \n",
    "def val_gen():\n",
    "    names=val_files\n",
    "    \n",
    "    X_batch, Y_batch=generator(names,labels)\n",
    "    while True:\n",
    "            #print('vgen')\n",
    "            yield (X_batch,Y_batch)\n",
    "\n",
    "def cv_val_gen():\n",
    "    #print('cvgen')\n",
    "    names=val_files\n",
    "    X_batch, Y_batch=generator(cv_val_files,cv_val_labels,True)\n",
    "    return X_batch,Y_batch\n",
    "def generator(names,labels,cv_val=False):\n",
    "    X_batch=[]\n",
    "    Y_batch=[]\n",
    "    embed=[]\n",
    "    for filename in names:\n",
    "        image=[]\n",
    "        img=Image.open('../'+filename)\n",
    "        X_batch.append(np.array(img))\n",
    "        d=0\n",
    "        if(len(labels[filename])>0):    \n",
    "            Y_batch.append(1)\n",
    "        else:   \n",
    "            Y_batch.append(0)\n",
    "     \n",
    "    X_batch=np.array(X_batch)\n",
    "    Y_batch=np.array(Y_batch)\n",
    "    #embed=create_inception_embedding(X_batch)\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     30,
     36
    ]
   },
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.cast(K.greater(K.clip(y_true * y_pred, 0, 1),0.5),'float32'))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.cast(K.greater(K.clip(y_true * y_pred, 0, 1),0.5),'float32'))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    #A=y_pred[:,1]*y_true[:,0]\n",
    "    #B=y_pred[:,2]*y_true[:,0]\n",
    "    #a=10*K.sum(K.square(A-y_true[:,1]))+10*K.sum(K.square(B-y_true[:,2]))+K.sum(K.square((1-y_pred[:,0])-(1-y_true[:,0])))\n",
    "    return keras.losses.binary_crossentropy(y_true,y_pred)\n",
    "\n",
    "def loss2(y_true, y_pred):\n",
    "    A=y_pred[:,0]*y_true[:,0]\n",
    "    B=y_pred[:,1]*y_true[:,0]\n",
    "    a=K.sum(K.square(A-y_true[:,1]))+K.sum(K.square(B-y_true[:,2]))\n",
    "    return a\n",
    "\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### below class was used for another validation set, much tougher than currently used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class AdditionalValidationSets(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [2, 3]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "\n",
    "            for i, result in enumerate(results):\n",
    "                if i == 0:\n",
    "                    valuename = validation_set_name + '_loss'\n",
    "                else:\n",
    "                    valuename = validation_set_name + '_' + self.model.metrics[i-1].__name__\n",
    "                print(valuename,result)\n",
    "                self.history.setdefault(valuename, []).append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsaver=keras.callbacks.ModelCheckpoint('/media/arpit/New Volume/Datasets/skylark/tmp/model-{epoch:02d}-{val_loss:.2f}.hdf5',verbose=True, mode='auto', period=10)\n",
    "adam=keras.optimizers.Adam(lr=0.0001)\n",
    "sgd = keras.optimizers.SGD(lr=0.0000001, decay=1e-8, momentum=0.9, nesterov=True)\n",
    "\n",
    "losses={\n",
    "    'dense_4':'binary_crossentropy',\n",
    "    #'dense_8':loss2\n",
    "}\n",
    "tensorboard = TensorBoard(log_dir=\"./output\")\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy',metrics=[f2_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(step):\n",
    "   initial_lrate = 0.00001\n",
    "   final_lrate=0.00001\n",
    "   lrate = (initial_lrate-final_lrate)*step/300\n",
    "   return initial_lrate\n",
    "\n",
    "lrate = keras.callbacks.LearningRateScheduler(exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Train model      \n",
    "\n",
    "model.fit_generator(train_gen(batch_size), epochs=300,callbacks=[modelsaver,tensorboard,lrate],steps_per_epoch=40,validation_data=val_gen(),validation_steps=1, max_queue_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(train_gen(50))\n",
    "Y=model.predict(x)\n",
    "Y=np.round(Y).astype(int)\n",
    "if Y[0]>0.5:\n",
    "    print(\"found\",Y[0])\n",
    "\n",
    "plt.imshow(x[0])\n",
    "print(precision_score(y,Y),recall_score(y,Y),f1_score(y,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRec(vec,Y):\n",
    "    index=np.argsort(Y,axis=0)[::-1]\n",
    "    print(Y[index][:15])\n",
    "    x=vec[index]\n",
    "    print(x.shape)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulateImage(img):\n",
    "    height=299*(1+(img.shape[0]//299))\n",
    "    width=299*(1+(img.shape[1]//299))\n",
    "    h=height-img.shape[0]\n",
    "    w=width-img.shape[1]\n",
    "    print( h//2, h-(h//2), w//2, w-(w//2))\n",
    "    image = cv2.copyMakeBorder( img, h//2, h-(h//2), w//2, w-(w//2), cv2.BORDER_CONSTANT)\n",
    "    vec=[]\n",
    "    for x in range(width//299):\n",
    "        for y in range(height//299):\n",
    "            img=image[299*y:299*(y+1),299*x:299*(x+1)]\n",
    "            vec.append(img)\n",
    "            \n",
    "    return np.array(vec)\n",
    "\n",
    "name='DJI_0036'\n",
    "vec=manipulateImage(np.array(Image.open('../CV-Assignment-Dataset/'+name+'.JPG'))/255)\n",
    "\n",
    "\n",
    "vec=plotRec(vec,model.predict(vec))\n",
    "i=0\n",
    "print(vec.shape)\n",
    "f=plt.figure(figsize=(10,15*5))\n",
    "i=1\n",
    "for x in vec[:15]:\n",
    "    f.add_subplot(15,1,i)\n",
    "    plt.imshow(x[0])\n",
    "    i+=1\n",
    "\n",
    "    #     time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
